<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zachary Olkin – Research</title>
    <link>http://localhost:1313/research/</link>
    <description>Recent content in Research on Zachary Olkin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	  <atom:link href="http://localhost:1313/research/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Autonomous Quadrotor Trajectory Planning and Control for In-Flight Aerial Vehicle Capture</title>
      <link>http://localhost:1313/research/aerial_capture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/research/aerial_capture/</guid>
      <description>
        
        
        &lt;h2&gt;Abstract&lt;span class=&#34;absolute -mt-20&#34; id=&#34;abstract&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#abstract&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This paper develops a path planning and control architecture for an interceptor air vehicle designed to capture small target UAVs, which are assumed to be at rest or moving slowly. The proposed architecture has three main parts. First, a geometric path planner is developed to determine the trajectory for the quadrotor to travel from its initial location to the rendezvous location. The second component of the architecture is a minimal-time thrust profile generation algorithm. The algorithm represents the quadrotor’s acceleration as a B-Spline and uses the convex-hull property of the spline to transform the constraints into functions of the control points so an optimization problem can be formulated to minimize time to capture. Lastly, a low-level controller tracks the orientation and thrust commands. In this architecture, the thrust profile and geometric path are generated independently. The proposed planning and control architecture provide one key advantage over alternative optimal control approaches: by separating the thrust profile generation from the geometric path generation, the minimal time trajectory generation problem has fewer variables and constraints to calculate, thus allowing on-board calculation of the thrust profile. Results are presented demonstrating aerial capture of targets in simulation.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;http://localhost:1313/research/images/geo_path_vectors.png&#34; title=&#34;Generated trajectory. The vehichle to capture is the ellipsoid in the center, and the controlled vehicle starts on the ground below it.&#34; alt=&#34;&#34; loading=&#34;lazy&#34; /&gt;
    &lt;figcaption&gt;Generated trajectory. The vehichle to capture is the ellipsoid in the center, and the controlled vehicle starts on the ground below it.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;h2&gt;Additional Info&lt;span class=&#34;absolute -mt-20&#34; id=&#34;additional-info&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#additional-info&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Zachary Olkin, Jonathan Rogers&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Location&lt;/strong&gt;: AREAL, Georgia Tech&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: May, 2020 - January 2021&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9438242&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Zolkin1/quad-sim-vehicle-capture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Bilevel Optimization for Real-Time Control with Application to Locomotion Gait Generation</title>
      <link>http://localhost:1313/research/bilevel-gait-gen/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/research/bilevel-gait-gen/</guid>
      <description>
        
        
        &lt;h2&gt;Abstract&lt;span class=&#34;absolute -mt-20&#34; id=&#34;abstract&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#abstract&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Model Predictive Control (MPC) is a common tool for the control of nonlinear, real-world systems, such as legged robots. However, solving MPC quickly enough to enable its use in real-time is often challenging. One common solution is given by real-time iterations, which does not solve the MPC problem to convergence, but rather close enough to give an approximate solution. In this paper, we extend this idea to a bilevel control framework where a &amp;ldquo;high-level&amp;rdquo; optimization program modifies a controller parameter of a &amp;ldquo;low-level&amp;rdquo; MPC problem which generates the control inputs and desired state trajectory. We propose an algorithm to iterate on this bilevel program in real-time and provide conditions for its convergence and improvements in stability. We then demonstrate the efficacy of this algorithm by applying it to a quadrupedal robot where the high-level problem optimizes a contact schedule in real-time. We show through simulation that the algorithm can yield improvements in disturbance rejection and optimality, while creating qualitatively new gaits.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;http://localhost:1313/research/images/bilevel_gait_gen.jpg&#34; title=&#34;Structure of the bilevel optimization. The MPC uses parameters from the high-level optimization and outputs the control inputs and state trajectory. When applied to the quadruped, the high-level parameter is the contact schedule. The green dots indicate contact with the ground and show how the contact schedule changes over time.&#34; alt=&#34;&#34; loading=&#34;lazy&#34; /&gt;
    &lt;figcaption&gt;Structure of the bilevel optimization. The MPC uses parameters from the high-level optimization and outputs the control inputs and state trajectory. When applied to the quadruped, the high-level parameter is the contact schedule. The green dots indicate contact with the ground and show how the contact schedule changes over time.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;h2&gt;Additional Info&lt;span class=&#34;absolute -mt-20&#34; id=&#34;additional-info&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#additional-info&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Zachary Olkin, Aaron Ames&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; CDC 2024 (Conference on Decision and Control)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Location of work&lt;/strong&gt;: Amber Lab, Caltech&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dates of work&lt;/strong&gt;: October 2023 - April 2024&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2409.12366&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Zolkin1/bilevel-gait-gen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning</title>
      <link>http://localhost:1313/research/clf_rl_running/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/research/clf_rl_running/</guid>
      <description>
        
        
        &lt;h2&gt;Abstract&lt;span class=&#34;absolute -mt-20&#34; id=&#34;abstract&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#abstract&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Achieving highly dynamic behaviors on humanoid robots, such as running, requires controllers that are both robust and precise, and hence difficult to design. Classical control methods offer valuable insight into how such systems can stabilize themselves, but synthesizing real-time controllers for nonlinear and hybrid dynamics remains challenging. Recently, reinforcement learning (RL) has gained popularity for locomotion control due to its ability to handle these complex dynamics. In this work, we embed ideas from nonlinear control theory, specifically control Lyapunov functions (CLFs), along with optimized dynamic reference trajectories into the reinforcement learning training process to shape the reward. This approach, CLF-RL, eliminates the need to handcraft and tune heuristic reward terms, while simultaneously encouraging certifiable stability and providing meaningful intermediate rewards to guide learning. By grounding policy learning in dynamically feasible trajectories, we expand the robot’s dynamic capabilities and enable running that includes both flight and single support phases. The resulting policy operates reliably on a treadmill and in outdoor environments, demonstrating robustness to disturbances applied to the torso and feet. Moreover, it achieves accurate global reference tracking utilizing only on-board sensors, making a critical step toward integrating these dynamic motions into a full autonomy stack.&lt;/p&gt;
&lt;h2&gt;Video&lt;span class=&#34;absolute -mt-20&#34; id=&#34;video&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#video&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;

    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/zCtDQuZAomI?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h2&gt;Figures&lt;span class=&#34;absolute -mt-20&#34; id=&#34;figures&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#figures&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;http://localhost:1313/research/images/clf_rl_running_hero_fig.jpg&#34; title=&#34;Overview of our approach. Trajectory optimization through each of the hybrid domains generates desired trajectories, which are used to construct a CLF-based reward. An RL policy is trained in simulation with this reward and deployed on a real humanoid robot.&#34; alt=&#34;&#34; loading=&#34;lazy&#34; /&gt;
    &lt;figcaption&gt;Overview of our approach. Trajectory optimization through each of the hybrid domains generates desired trajectories, which are used to construct a CLF-based reward. An RL policy is trained in simulation with this reward and deployed on a real humanoid robot.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2&gt;Additional Info&lt;span class=&#34;absolute -mt-20&#34; id=&#34;additional-info&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#additional-info&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Zachary Olkin, Kejun Li, William Compton, Aaron Ames.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; Submitted to ICRA 2026&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Location of work&lt;/strong&gt;: Amber Lab, Caltech&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dates of work&lt;/strong&gt;: August 2025 - September 2025&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: arxiv (Coming soon).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Zolkin1/robot_rl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL + Deployment Code&lt;/a&gt;, &lt;a href=&#34;https://github.com/Caltech-AMBER/traj_opt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trajopt Code&lt;/a&gt;. The first repo hosts all the RL code and the code to deploy on the hardware. The second repo has the code for generating the trajectories.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>CLF-RL: Control Lyapunov Function Guided Reinforcement Learning</title>
      <link>http://localhost:1313/research/clf_rl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/research/clf_rl/</guid>
      <description>
        
        
        &lt;h2&gt;Abstract&lt;span class=&#34;absolute -mt-20&#34; id=&#34;abstract&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#abstract&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Reinforcement learning (RL) has shown promise in generating robust locomotion policies for bipedal robots, but often suffers from tedious reward design and sensitivity to poorly shaped objectives. In this work, we propose a structured reward shaping framework that leverages model-based trajectory generation and control Lyapunov functions (CLFs) to guide policy learning. We explore two model-based planners for generating reference trajectories: a reduced-order linear inverted pendulum (LIP) model for velocity-conditioned motion planning, and a precomputed gait library based on hybrid zero dynamics (HZD) using full-order dynamics. These planners define desired end-effector and joint trajectories, which are used to construct CLF-based rewards that penalize tracking error and encourage rapid convergence. This formulation provides meaningful intermediate rewards, and is straightforward to implement once a reference is available. Both the reference trajectories and CLF shaping are used only during training, resulting in a lightweight policy at deployment. We validate our method both in simulation and through extensive real-world experiments on a Unitree G1 robot. CLF-RL demonstrates significantly improved robustness relative to the baseline RL policy and better performance than a classic tracking reward RL formulation.&lt;/p&gt;
&lt;h2&gt;Video&lt;span class=&#34;absolute -mt-20&#34; id=&#34;video&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#video&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;

    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/f8iuwgCZs3A?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h2&gt;Figures&lt;span class=&#34;absolute -mt-20&#34; id=&#34;figures&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#figures&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;http://localhost:1313/research/images/clf_rl_hero_fig.jpg&#34; title=&#34;Overview of our approach. A reference generator produces target trajectories, which are used to construct a CLF-based reward. An RL policy is trained in simulation with this reward and deployed on a real humanoid robot.&#34; alt=&#34;&#34; loading=&#34;lazy&#34; /&gt;
    &lt;figcaption&gt;Overview of our approach. A reference generator produces target trajectories, which are used to construct a CLF-based reward. An RL policy is trained in simulation with this reward and deployed on a real humanoid robot.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2&gt;Additional Info&lt;span class=&#34;absolute -mt-20&#34; id=&#34;additional-info&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#additional-info&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Kejun Li*, Zachary Olkin*, Yisong Yue, Aaron Ames (* indicates equal contribution).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Submitted to RA-L&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Location of work&lt;/strong&gt;: Amber Lab, Caltech&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dates of work&lt;/strong&gt;: June 2025 - August 2025&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2508.09354&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Zolkin1/robot_rl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL + Deployment Code&lt;/a&gt;, &lt;a href=&#34;https://github.com/Caltech-AMBER/traj_opt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trajopt Code&lt;/a&gt;. The first repo hosts all the RL code and the code to deploy on the hardware. The second repo has the code for generating the trajectories.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Custom Drone for First Responders</title>
      <link>http://localhost:1313/research/first_responders_drone/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/research/first_responders_drone/</guid>
      <description>
        
        
        &lt;h2&gt;Description&lt;span class=&#34;absolute -mt-20&#34; id=&#34;description&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#description&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;I was creating a custom built drone that is optimized for use by first responders. The goal was to build a drone that can take off after an upside down landing, stabilize its position indoors without GPS, and function in complete darkness all while gathering critical information for first responders. I generated requirements, wrote specifications, wrote code, designed and soldered electronics, and modeled, fabriacted, and built a custom drone. This was a solo project where I performed all of the design, implementation, and testing of the quadcopter. This project has been an amazing learning experience where I have improved my engineering skills and learned to make solutions that are robust and accurate.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;http://localhost:1313/research/images/first_responder_drone.jpg&#34; title=&#34;Version 1 of the first responder drone.&#34; alt=&#34;&#34; loading=&#34;lazy&#34; /&gt;
    &lt;figcaption&gt;Version 1 of the first responder drone.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;h2&gt;Additional Info&lt;span class=&#34;absolute -mt-20&#34; id=&#34;additional-info&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#additional-info&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Location&lt;/strong&gt;: AREAL, Georgia Tech&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Date&lt;/strong&gt;: January 2021 - May 2022&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Drone Dock Cruise Control</title>
      <link>http://localhost:1313/research/drone_dock/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/research/drone_dock/</guid>
      <description>
        
        
        &lt;h2&gt;Description&lt;span class=&#34;absolute -mt-20&#34; id=&#34;description&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#description&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;I modified a R/C car to have a cruise control system and log critical data. The overall goal of the research project is to land an autonomous quadcopter on a moving dock. The truck pictured below will have the landing dock mounted ontop. This cruise control system has a few modes: a &amp;ldquo;pass through&amp;rdquo; mode where the truck can be controlled normally, a mode in which the cruise control setpoint can be wirelessly changed, and a cruise control mode where a constant PWM width is passed into the ESC from the onboard computer. GPS data is logged onto a micro SD card for further analysis after the test runs. By using this system, the truck does not need to be driven during testing, and a constant velocity can be used for testing. This makes it easier to debug and run tests with the system.&lt;/p&gt;
&lt;p&gt;
    &lt;figure&gt;
    &lt;img src=&#34;../images/rc_car.jpg&#34; title=&#34;Finished drone dock showcasing electronics.&#34; alt=&#34;&#34; loading=&#34;lazy&#34; /&gt;
    &lt;figcaption&gt;Finished drone dock showcasing electronics.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;h2&gt;Additional Info&lt;span class=&#34;absolute -mt-20&#34; id=&#34;additional-info&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#additional-info&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Location&lt;/strong&gt;: AREAL Lab, Georgia Tech&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dates&lt;/strong&gt;: January 2021 - March 2021&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Indoor Flight Lab Synthetic GPS</title>
      <link>http://localhost:1313/research/ifl_gps/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/research/ifl_gps/</guid>
      <description>
        
        
        &lt;h2&gt;Description&lt;span class=&#34;absolute -mt-20&#34; id=&#34;description&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#description&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;I designed and tested a tool that takes in VICON motion caputre data and converts it to an equivalent stream of data formatted as a GPS. Using this tool, any flight controller that would expect GPS data can be used in the Indoor Flight Lab (IFL) at Georgia Tech with no modifications. The tool can be easily customized to fit anyone&amp;rsquo;s needs. Additional noise can be artifically added to the VICON data to better emulate GPS data. The rate at which the data is output and the format of the data (NMEA and UBX) can also be chosen.&lt;/p&gt;
&lt;p&gt;
    &lt;figure&gt;
    &lt;img src=&#34;../images/ifl_photon.jpg&#34; title=&#34;Synthetic GPS device connected to a flight controller in the IFL.&#34; alt=&#34;&#34; loading=&#34;lazy&#34; /&gt;
    &lt;figcaption&gt;Synthetic GPS device connected to a flight controller in the IFL.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;h2&gt;Additional Info&lt;span class=&#34;absolute -mt-20&#34; id=&#34;additional-info&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#additional-info&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Location&lt;/strong&gt;:  AREAL and Indoor Flight Lab (IFL), Georgia Tech&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dates&lt;/strong&gt;: January 2020 - January 2021&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Layered Nonlinear Model Predictive Control for Robust Stabilization of Hybrid Systems</title>
      <link>http://localhost:1313/research/layered_mpc_hybrid_robustness/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/research/layered_mpc_hybrid_robustness/</guid>
      <description>
        
        
        &lt;h2&gt;Abstract&lt;span class=&#34;absolute -mt-20&#34; id=&#34;abstract&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#abstract&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Computing the receding horizon optimal control of nonlinear hybrid systems is typically prohibitively slow, limiting real-time implementation. To address this challenge, we propose a layered Model Predictive Control (MPC) architecture for robust stabilization of hybrid systems. A high level &amp;ldquo;hybrid&amp;rdquo; MPC is solved at a slow rate to produce a stabilizing hybrid trajectory, potentially sub-optimally, including a domain and guard sequence.  This domain and guard sequence is passed to a low level &amp;ldquo;fixed mode&amp;rdquo; MPC which is a traditional, time-varying, state-constrained MPC that can be solved rapidly, e.g., using nonlinear programming (NLP) tools. A robust version of the fixed mode MPC is constructed by using tracking error tubes that are not guaranteed to have finite size for all time. Using these tubes, we demonstrate that the speed at which the fixed mode MPC is re-calculated is directly tied to the robustness of the system, thereby justifying the layered approach. Finally, simulation examples of a five link bipedal robot and a controlled nonlinear bouncing ball are used to illustrate the formal results.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;http://localhost:1313/research/images/layered_mpc_hybrid_fig.jpg&#34; title=&#34;The hybrid MPC layer determines a feasible path at a slow rate, and passes these modes to the fixed mode MPC. This MPC computes a robustly stabilizing solution at a higher rate. The trajectory and feed forward input is passed to the low level controller where it is tracked. We use properties of the low level controller to determine the size of the tube. The resulting control action is applied to the dynamics.&#34; alt=&#34;&#34; loading=&#34;lazy&#34; /&gt;
    &lt;figcaption&gt;The hybrid MPC layer determines a feasible path at a slow rate, and passes these modes to the fixed mode MPC. This MPC computes a robustly stabilizing solution at a higher rate. The trajectory and feed forward input is passed to the low level controller where it is tracked. We use properties of the low level controller to determine the size of the tube. The resulting control action is applied to the dynamics.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;h2&gt;Additional Info&lt;span class=&#34;absolute -mt-20&#34; id=&#34;additional-info&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#additional-info&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Zachary Olkin, Aaron Ames&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; ACC 2025 (American Control Conference)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Location of work&lt;/strong&gt;: Amber Lab, Caltech&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dates of work&lt;/strong&gt;: May 2024 - September 2024&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2503.12810v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Zolkin1/hybrid-mpc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;. Note that the code was not designed as a library. This is just the code to re-create the examples.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Locomotion on Constrained Footholds via Layered Architectures and Model Predictive Control</title>
      <link>http://localhost:1313/research/layered_mpc_control_through_contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/research/layered_mpc_control_through_contact/</guid>
      <description>
        
        
        &lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2&gt;Abstract&lt;span class=&#34;absolute -mt-20&#34; id=&#34;abstract&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#abstract&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Computing stabilizing and optimal control actions for legged locomotion in real time is difficult due to the nonlinear, hybrid, and high dimensional nature of these robots. The hybrid nature of the system introduces a combination of discrete and continuous variables which causes issues for numerical optimal control. To address these challenges, we propose a layered architecture that separates the choice of discrete variables and a smooth Model Predictive Controller (MPC). The layered formulation allows for online flexibility and optimality without sacrificing real-time performance through a combination of gradient-free and gradient-based methods. The architecture leverages a sampling-based method for determining discrete variables, and a classical smooth MPC formulation using these fixed discrete variables. We demonstrate the results on a quadrupedal robot stepping over gaps and onto terrain with varying heights. In simulation, we demonstrate the controller on a humanoid robot for gap traversal. The layered approach is shown to be more optimal and reliable than common heuristic-based approaches and faster to compute than pure sampling methods.&lt;/p&gt;
&lt;h2&gt;Video&lt;span class=&#34;absolute -mt-20&#34; id=&#34;video&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#video&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;

    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/pTIWqsFODQk?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h2&gt;Pictures&lt;span class=&#34;absolute -mt-20&#34; id=&#34;pictures&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#pictures&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;http://localhost:1313/research/images/humanoids_layered_mpc_hero_fig_v2.jpg&#34; title=&#34;Demonstration of the layered controller dynamically navigating over complex terrain and locomoting effectively on flat ground. The controller is both versatile in the terrain and actions it can stabilize and fast enough for real-time control.&#34; alt=&#34;&#34; loading=&#34;lazy&#34; /&gt;
    &lt;figcaption&gt;Demonstration of the layered controller dynamically navigating over complex terrain and locomoting effectively on flat ground. The controller is both versatile in the terrain and actions it can stabilize and fast enough for real-time control.&lt;/figcaption&gt;
  &lt;/figure&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2&gt;Additional Info&lt;span class=&#34;absolute -mt-20&#34; id=&#34;additional-info&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#additional-info&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Zachary Olkin, Aaron Ames&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conference:&lt;/strong&gt; Humanoids 2025&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Location of work&lt;/strong&gt;: Amber Lab, Caltech&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dates of work&lt;/strong&gt;: September 2024 - April 2025&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2506.09979&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Zolkin1/sample-contact-walking&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repo 1&lt;/a&gt;, &lt;a href=&#34;https://github.com/Zolkin1/torc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repo 2&lt;/a&gt;. The second repo (TORC) has the MPC code as a library and the first repo is the code that actually runs the simulation and hardware.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
